PE 0: MPICH processor detected:
PE 0:   AMD Genoa (25:17:1) (family:model:stepping)
MPI VERSION    : CRAY MPICH version 8.1.28.15 (ANL base 3.4a2)
MPI BUILD INFO : Wed Nov 15 21:59 2023 (git hash 1cde46f) (CH4)
PE 0: MPICH environment settings =====================================
PE 0:   MPICH_ENV_DISPLAY                              = 1
PE 0:   MPICH_VERSION_DISPLAY                          = 1
PE 0:   MPICH_ABORT_ON_ERROR                           = 0
PE 0:   MPICH_CPUMASK_DISPLAY                          = 0
PE 0:   MPICH_STATS_DISPLAY                            = 0
PE 0:   MPICH_RANK_REORDER_METHOD                      = 1
PE 0:   MPICH_RANK_REORDER_DISPLAY                     = 0
PE 0:   MPICH_MEMCPY_MEM_CHECK                         = 0
PE 0:   MPICH_USE_SYSTEM_MEMCPY                        = 1
PE 0:   MPICH_OPTIMIZED_MEMCPY                         = 0
PE 0:   MPICH_ALLOC_MEM_PG_SZ                          = 4096
PE 0:   MPICH_ALLOC_MEM_POLICY                         = PREFERRED
PE 0:   MPICH_ALLOC_MEM_AFFINITY                       = SYS_DEFAULT
PE 0:   MPICH_MALLOC_FALLBACK                          = 0
PE 0:   MPICH_MEM_DEBUG_FNAME                          = 
PE 0:   MPICH_INTERNAL_MEM_AFFINITY                    = SYS_DEFAULT
PE 0:   MPICH_NO_BUFFER_ALIAS_CHECK                    = 0
PE 0:   MPICH_COLL_SYNC                                = 0
PE 0:   MPICH_SINGLE_HOST_ENABLED                        = 1
PE 0:   MPICH_USE_PERSISTENT_TOPS                      = 0
PE 0:   MPICH_DISABLE_PERSISTENT_RECV_TOPS             = 0
PE 0:   MPICH_MAX_TOPS_COUNTERS                        = 0
PE 0:   MPICH_ENABLE_ACTIVE_WAIT                       = 0
PE 0: MPICH/RMA environment settings =================================
PE 0:   MPICH_RMA_MAX_PENDING                          = 128
PE 0:   MPICH_RMA_SHM_ACCUMULATE                       = 0
PE 0: MPICH/Dynamic Process Management environment settings ==========
PE 0:   MPICH_DPM_DIR                                  = 
PE 0:   MPICH_LOCAL_SPAWN_SERVER                       = 0
PE 0:   MPICH_SPAWN_USE_RANKPOOL                       = 0
PE 0: MPICH/SMP environment settings =================================
PE 0:   MPICH_SMP_SINGLE_COPY_MODE                     = CMA
PE 0:   MPICH_SMP_SINGLE_COPY_SIZE                     = 8192
PE 0:   MPICH_SHM_PROGRESS_MAX_BATCH_SIZE              = 8
PE 0: MPICH CH4 OFI detected 2 NICs/node on host x3100c0s17b0n0
PE 0: MPICH CH4 OFI netmod using cxi provider (domain_name=cxi0, src_addr=0x1008)
PE 0: Selected traffic class: [TC_BEST_EFFORT, 512]
PE 0: MPICH/OFI environment settings =================================
PE 0:   MPICH_OFI_USE_PROVIDER                         = cxi
PE 0:   MPICH_OFI_VERBOSE                              = 1
PE 0:   MPICH_OFI_NIC_VERBOSE                          = 0
PE 0:   FI_CXI_RDZV_THRESHOLD                          = 16384
PE 0:   FI_CXI_RDZV_EAGER_SIZE                         = 2048
PE 0:   FI_CXI_DEFAULT_CQ_SIZE                         = 131072
PE 0:   FI_CXI_DEFAULT_TX_SIZE                         = 1024
PE 0:   FI_CXI_OFLOW_BUF_SIZE                          = 12582912
PE 0:   FI_CXI_OFLOW_BUF_COUNT                         = 3
PE 0:   FI_CXI_RX_MATCH_MODE                           = hardware
PE 0:   FI_CXI_REQ_BUF_MIN_POSTED                      = 6
PE 0:   FI_CXI_REQ_BUF_SIZE                            = 12582912
PE 0:   FI_CXI_REQ_BUF_MAX_CACHED                      = 0
PE 0:   FI_MR_CACHE_MAX_SIZE                           = -1
PE 0:   FI_MR_CACHE_MAX_COUNT                          = 524288
PE 0:   MPICH_OFI_DEFAULT_TCLASS                       = TC_BEST_EFFORT
PE 0:   MPICH_OFI_TCLASS_ERRORS                        = warn
PE 0:   MPICH_OFI_CXI_PID_BASE                         = 0
PE 0:   MPICH_OFI_USE_SCALABLE_STARTUP                 = 1
PE 0:   MPICH_OFI_CXI_COUNTER_REPORT                   = 1
PE 0:   MPICH_OFI_CXI_COUNTER_VERBOSE                  = 0
PE 0:   MPICH_OFI_CXI_COUNTER_FILE                     = NULL
PE 0:   MPICH_OFI_CXI_COUNTER_REPORT_FILE              = NULL
PE 0:   MPICH_OFI_NIC_POLICY                           = BLOCK
PE 0:   MPICH_OFI_NIC_MAPPING                          = NULL
PE 0:   MPICH_OFI_NUM_NICS                             = NULL
PE 0:   MPICH_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS     = -1
PE 0:   MPICH_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS        = -1
PE 0:   MPICH_OFI_RMA_STARTUP_CONNECT                  = 0
PE 0:   MPICH_OFI_SKIP_NIC_SYMMETRY_TEST               = 0
PE 0:   MPICH_OFI_STARTUP_CONNECT                      = 0
PE 0: MPICH/COLLECTIVE environment settings ==========================
PE 0:   MPICH_COLL_OPT_OFF                             = 0
PE 0:   MPICH_BCAST_ONLY_TREE                          = 1
PE 0:   MPICH_BCAST_INTERNODE_RADIX                    = 4
PE 0:   MPICH_BCAST_INTRANODE_RADIX                    = 4
PE 0:   MPICH_ALLTOALL_SHORT_MSG                       = 64-512
PE 0:   MPICH_ALLTOALL_SYNC_FREQ                       = 1-24
PE 0:   MPICH_ALLTOALL_BLK_SIZE                        = 16384
PE 0:   MPICH_ALLTOALL_CHUNKING_MAX_NODES              = 90
PE 0:   MPICH_ALLTOALLV_THROTTLE                       = 8
PE 0:   MPICH_ALLGATHER_VSHORT_MSG                     = 8192-16384
PE 0:   MPICH_ALLGATHERV_VSHORT_MSG                    = 8192-16384
PE 0:   MPICH_GATHERV_SHORT_MSG                        = 131072
PE 0:   MPICH_GATHERV_MIN_COMM_SIZE                    = 64
PE 0:   MPICH_GATHERV_MAX_TMP_SIZE                     = 536870912
PE 0:   MPICH_GATHERV_SYNC_FREQ                        = 16
PE 0:   MPICH_IGATHERV_MIN_COMM_SIZE                   = 1000
PE 0:   MPICH_IGATHERV_SYNC_FREQ                       = 100
PE 0:   MPICH_IGATHERV_RAND_COMMSIZE                   = 2048
PE 0:   MPICH_IGATHERV_RAND_RECVLIST                   = 0
PE 0:   MPICH_SCATTERV_SHORT_MSG                       = 2048-8192
PE 0:   MPICH_SCATTERV_MIN_COMM_SIZE                   = 64
PE 0:   MPICH_SCATTERV_MAX_TMP_SIZE                    = 536870912
PE 0:   MPICH_SCATTERV_SYNC_FREQ                       = 16
PE 0:   MPICH_SCATTERV_SYNCHRONOUS                     = 0
PE 0:   MPICH_ALLREDUCE_MAX_SMP_SIZE                   = 262144
PE 0:   MPICH_ALLREDUCE_BLK_SIZE                       = 716800
PE 0:   MPICH_GPU_ALLGATHER_VSHORT_MSG_ALGORITHM       = 1
PE 0:   MPICH_GPU_ALLREDUCE_USE_KERNEL                 = 0
PE 0:   MPICH_GPU_COLL_STAGING_BUF_SIZE                = 1048576
PE 0:   MPICH_GPU_ALLREDUCE_STAGING_THRESHOLD          = 256
PE 0:   MPICH_ALLREDUCE_NO_SMP                         = 0
PE 0:   MPICH_REDUCE_NO_SMP                            = 0
PE 0:   MPICH_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE = 524288
PE 0:   MPICH_REDUCE_SCATTER_MAX_COMMSIZE              = 1000
PE 0:   MPICH_SHARED_MEM_COLL_OPT                      = 1
PE 0:   MPICH_SHARED_MEM_COLL_NCELLS                   = 8
PE 0:   MPICH_SHARED_MEM_COLL_CELLSZ                   = 256
PE 0: MPICH MPIIO environment settings ===============================
PE 0:   MPICH_MPIIO_HINTS_DISPLAY                      = 0
PE 0:   MPICH_MPIIO_HINTS                              = NULL
PE 0:   MPICH_MPIIO_ABORT_ON_RW_ERROR                  = disable
PE 0:   MPICH_MPIIO_CB_ALIGN                           = 2
PE 0:   MPICH_MPIIO_DVS_MAXNODES                       = -1
PE 0:   MPICH_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY       = 0
PE 0:   MPICH_MPIIO_AGGREGATOR_PLACEMENT_STRIDE        = -1
PE 0:   MPICH_MPIIO_MAX_NUM_IRECV                      = 50
PE 0:   MPICH_MPIIO_MAX_NUM_ISEND                      = 50
PE 0:   MPICH_MPIIO_MAX_SIZE_ISEND                     = 10485760
PE 0:   MPICH_MPIIO_OFI_STARTUP_CONNECT                = disable
PE 0:   MPICH_MPIIO_OFI_STARTUP_NODES_AGGREGATOR        = 2
PE 0: MPICH MPIIO statistics environment settings ====================
PE 0:   MPICH_MPIIO_STATS                              = 0
PE 0:   MPICH_MPIIO_TIMERS                             = 0
PE 0:   MPICH_MPIIO_WRITE_EXIT_BARRIER                 = 1
PE 0: MPICH Thread Safety settings ===================================
PE 0:   MPICH_ASYNC_PROGRESS                           = 0
PE 0:   MPICH_OPT_THREAD_SYNC                          = 1
PE 0:   rank 0 required = single, was provided = single
PE 0: MPICH CH4 OFI CXI counters initialized

 BerkeleyGW version 4.0+git 8511836-dirty by Jacob Clary <jacl0659@ed1.ib0.cm.hpc.nrel.gov> at 2024-03-13 16:03:19 -0600
 Epsilon code, Complex version, run on 10-Aug-2025 at 13:03:46 -0600

                                                                 ..o.          
                                                                .oxxo.         
                                                               .oxxxxo...      
                                                               oxxxxxxo.       
                                                              .oxxxxxxx.       
                                                              .ooooooxxo..     
                                                              .oooooooxo..     
                                                              .oooooxxo...     
                                                       .........oxooo......    
                                                 ............................  
                                             ................................. 
                                          .................................... 
           .          ..oo. ....  .................................oooxxxxxxxo.
     .............oxxxx@ox@@@x@x.....................o...........ooooooooooxx. 
    .o.........oox@x.oo........xxx@@............ooxxxxo..........ooooxxxxxoxo  
    .x........x@xxo...............o@xxo........oxxx@@@xoooooooooooooooxxxo...  
    .o......ox@@o..................oox@o.....ooxxx@xoooxxxxxxxoooooooooooo.... 
    o..ooooo@@xoooo....ooo...........x@o.....ooxxxxo   .oxxxxxxxxxxooooooo.... 
    . .oooo@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@....ooooox.     .oxx@@@@@xxoo........ 
      .ooooxxxxxxxooooooxooooooooooooooxo...oooooxx.        ..ox@xxxoo.........
      .ooooooxxxx@xoooooooooooooxxoooooooooooxxxxx.            .oxxooooooooxxo.
     .oooooxxxxx@@@xxxxxxxxxxxxxxxxxxxxxxxxoxxxxo.                .oxxxxxxxxo. 
   ....oooxxxxx@@@xo..oxxx@@@@@@xxxxoxxoooooooxx.                  .oxxxoo..   
  .....ooxxxx@@xo.       ........    .ooooooooxxo                              
  ..oooxxxx@@@o                       .oooooooxoo.                             
  ....oxooxxxxx.                       .ooo..oooo.                             
  .....o.ooxxxxxo.                     .oooooooxo.                             
 ......ooooxxxxxxo.                     .ooooooxoo..                           
........ooxxxxxxxo..                     .o....oxoo...                         
.......ooooxxxxxxxo.                     ........oooo.                         
.ooooooo..ooxxxxoooo.                    .........ooo...                       
..oxo...ooooxxxoooo..                    .ooo......oooo...                     
  .ooooo....o.                            .oxxxoo....ooo....                   
    .oooooo...                              ...ooooo...ooo..                   
       ...                                     .oo.......                      
                                               ....ooo...                      
                      __              __                                       
 ______              [  |            [  |                 ._____  _          _ 
|_   _               | |  _          | |                /  ___ | |        | |
  | |_) | .---.  _. _.| | / |  .---.  | | .---.  _    _ / /    _|   /  / / 
  |  __'./ /__\[ /`_| '' <  / /__\ | |/ /__\|   | | |   _____  /  / /  
 _| |__| | __. | |   | |`  | ___. | || ___.  / /  .___| |    /  /   
|_______/ .__./[_]  [__|  _] .__./[___].__./    /   .____./    /  /    
                                                  / /                          
                                                 /_/                           
 --------------------------------------------------------------------------------
 Please cite the main BerkeleyGW paper when using results from this calculation:

 Jack Deslippe, Georgy Samsonidze, David A. Strubbe, Manish Jain, Marvin L.
 Cohen, and Steven G. Louie, "BerkeleyGW: A Massively Parallel Computer Package
 for the Calculation of the Quasiparticle and Optical Properties of Materials
 and Nanostructures," Comput. Phys. Commun. 183, 1269 (2012)

 Aditionally, PLEASE CITE ALL PAPERS REPORTED AT THE END OF THIS CALCULATION.
 --------------------------------------------------------------------------------

 Running MPI version (parallel)
 Running with 384 MPI task(s)
 Using OpenMP. Number of threads per MPI task: 32
 Number of threads used for sorting routines: 1

 Compilation flags:
 - Compiler: NVHPC
 - Para. flags: MPI, OMP
 - Math  flags: USESCALAPACK, UNPACKED, USEFFTW3, HDF5
 - Debug flags: 

 Running with verbosity level 1 (default).

 We will start a new calculation from scratch.

 Experimental Features:
  - FFTs Optimization Level: 0
  - Use HDF5 Parallel IO: T

 
 We`ll perform a uniform sampling of the full BZ.
 
 Computing the static inverse dielectric matrix

 We are using matrix communication scheme

 We are using non-blocking cyclic communication scheme

 Using block algorithm over valence bands

 We are communicating via MPI

 We are using no truncation
 Reading header of WFN.h5
 Highest occupied band (unshifted grid) = 1996
 Valence max    (unshifted grid) = 6.233852 eV
 Conduction min (unshifted grid) = 6.597934 eV
 Middle energy  (unshifted grid) = 6.415893 eV
 Fermi  energy  (unshifted grid) = 6.415893 eV

 Reading header of WFNq.h5
 Highest occupied band (shifted grid) = 1996

 Calculation parameters:
 - Cutoff of the dielectric matrix (Ry): 8.00
 - Total number of bands in the calculation: 29346
 - Number of fully occupied valence bands: 1996
 - Number of partially occ. conduction bands: 0
 - Monkhorst-Pack q-grid for epsilon(q): 1 1 1
 
 Running with 8 valence pools
 Number of conduction bands per processor: 570
 Number of valence bands per processor: 250

 Memory available: 124063.3 MB per PE
 Memory required for execution: 43644.4 MB per PE
 Memory required for vcoul: 0.0 MB per PE

 Valence block alogorithm info
 Max memory allowed per valence block (Gb):    80.00
 Memory for a single valence band (Gb):         0.94
 Number of valence bands per block:              84

 Number of electrons per unit cell (from ifmax) = 3992.000000
 Number of electrons per unit cell (from occupations) = 3992.000000
 Plasma Frequency = 1.218925 Ry

 Reading using H5FD_MPIO_INDEPENDENT.
 Using communication style:  1
 
 Reading using H5FD_MPIO_INDEPENDENT.
 Using communication style:  1
 
 Initializing eps0mat.h5
 - Screened Coulomb cutoff: 8.000 Ry
 - Total number of bands: 29346
 - Number of q-points: 1

 Scissors parameters:
 - Valence:    es =   0.0000 eV, e0 =   0.0000 eV, edel =   0.0000
 - Conduction: es =   0.0000 eV, e0 =   0.0000 eV, edel =   0.0000

 Q0 and |Q0| =  0.000000  0.000000  0.200000  0.024493

 GPU acceleration is : ENABLED
 
 Algorithms used:
 - mtxel         : OPENACC_ALGO
 - chi_summation : OPENACC_ALGO
 
 You have a slightly shifted q0 vector and a semiconductor.
 So, reading from WFNq.
 Reading header of WFNq.h5
 Highest occupied band (shifted grid) = 1996
 Reading using H5FD_MPIO_INDEPENDENT.
 Using communication style:  1
 
 Summary of the WFN files:
 - Number of k-points in WFN: 1
 - Number of k-points in WFNq: 1
 - Number of k-points in the full BZ of WFN: 1

================================================================================
 13:04:46   Dealing with q =  0.000000  0.000000  0.200000                 1 / 1
================================================================================

 This is the special q->0 point.
 Rank of the polarizability matrix (nmtx): 51627
 
 BLACS processor grid:  16 x  24; BLOCKSIZE =   32
 
 Number of k-points in the irreducible BZ(q) (nrk): 1
 
 ========================================================
 == Start Building Polarizability Using:     3 Blocks. ==
 ========================================================
 
 Valence Block Number:     1 of     3 || Pass     1 of     1
 -----------------------------------------------------------
 
 Using GPU support via OpenACC/OMP-Target implementation
 Using batch implementation, batch size:    20
 
 Started calculation of matrix elements with 84 transition(s) at 13:04:48.
 [ 13:04:48 |   0% ] transition  1 / 84.
 [ 13:04:49 |   2% ] transition  3 / 84, remaining: 13 s.
 [ 13:04:50 |  10% ] transition  9 / 84, remaining: 12 s.
 [ 13:04:51 |  19% ] transition 17 / 84, remaining: 10 s.
 [ 13:04:52 |  30% ] transition 26 / 84, remaining: 9 s.
 [ 13:04:54 |  39% ] transition 34 / 84, remaining: 8 s.
 [ 13:04:55 |  50% ] transition 43 / 84, remaining: 6 s.
 [ 13:04:56 |  60% ] transition 51 / 84, remaining: 5 s.
 [ 13:04:57 |  69% ] transition 59 / 84, remaining: 4 s.
 [ 13:04:59 |  80% ] transition 68 / 84, remaining: 3 s.
 [ 13:05:00 |  89% ] transition 76 / 84, remaining: 1 s.
 Finished calculation of matrix elements at 13:05:01.
 Elapsed time: 13 s.
 
 CHI summation GPU algorithm offloading all matrix elements to device.
 
 Started building polarizability matrix with 384 processor(s) at 13:05:02.
 [ 13:05:04 |   0% ] processor   1 / 384.
 [ 13:05:04 |   1% ] processor   3 / 384, remaining: 48 s.
 [ 13:05:09 |  10% ] processor  39 / 384, remaining: 43 s.
 [ 13:05:13 |  20% ] processor  77 / 384, remaining: 38 s.
 [ 13:05:18 |  30% ] processor 116 / 384, remaining: 33 s.
 [ 13:05:23 |  40% ] processor 154 / 384, remaining: 29 s.
 [ 13:05:28 |  50% ] processor 192 / 384, remaining: 24 s.
 [ 13:05:33 |  60% ] processor 231 / 384, remaining: 19 s.
 [ 13:05:37 |  70% ] processor 269 / 384, remaining: 15 s.
 [ 13:05:42 |  80% ] processor 308 / 384, remaining: 10 s.
 [ 13:05:47 |  90% ] processor 346 / 384, remaining: 5 s.
 [ 13:05:52 | 100% ] processor 384 / 384, remaining: 0 s.
 Finished building polarizability matrix at 13:05:52.
 Elapsed time: 50 s.
 
 Done With Block Number:     1
 -----------------------------
 
 
 Valence Block Number:     2 of     3 || Pass     1 of     1
 -----------------------------------------------------------
 
 Using GPU support via OpenACC/OMP-Target implementation
 Using batch implementation, batch size:    20
 
 Started calculation of matrix elements with 84 transition(s) at 13:05:54.
 [ 13:05:54 |   0% ] transition  1 / 84.
 [ 13:05:55 |   2% ] transition  3 / 84, remaining: 12 s.
 [ 13:05:56 |  10% ] transition  9 / 84, remaining: 11 s.
 [ 13:05:57 |  19% ] transition 17 / 84, remaining: 10 s.
 [ 13:05:58 |  30% ] transition 26 / 84, remaining: 9 s.
 [ 13:05:59 |  39% ] transition 34 / 84, remaining: 8 s.
 [ 13:06:01 |  50% ] transition 43 / 84, remaining: 6 s.
 [ 13:06:02 |  60% ] transition 51 / 84, remaining: 5 s.
 [ 13:06:04 |  69% ] transition 59 / 84, remaining: 4 s.
 [ 13:06:05 |  80% ] transition 68 / 84, remaining: 3 s.
 [ 13:06:06 |  89% ] transition 76 / 84, remaining: 1 s.
 Finished calculation of matrix elements at 13:06:08.
 Elapsed time: 14 s.
 
 CHI summation GPU algorithm offloading all matrix elements to device.
 
 Started building polarizability matrix with 384 processor(s) at 13:06:09.
 [ 13:06:10 |   0% ] processor   1 / 384.
 [ 13:06:10 |   1% ] processor   3 / 384, remaining: 51 s.
 [ 13:06:15 |  10% ] processor  39 / 384, remaining: 43 s.
 [ 13:06:20 |  20% ] processor  77 / 384, remaining: 38 s.
 [ 13:06:24 |  30% ] processor 116 / 384, remaining: 33 s.
 [ 13:06:29 |  40% ] processor 154 / 384, remaining: 29 s.
 [ 13:06:34 |  50% ] processor 192 / 384, remaining: 24 s.
 [ 13:06:39 |  60% ] processor 231 / 384, remaining: 19 s.
 [ 13:06:44 |  70% ] processor 269 / 384, remaining: 15 s.
 [ 13:06:49 |  80% ] processor 308 / 384, remaining: 10 s.
 [ 13:06:54 |  90% ] processor 346 / 384, remaining: 5 s.
 [ 13:06:59 | 100% ] processor 384 / 384, remaining: 0 s.
 Finished building polarizability matrix at 13:06:59.
 Elapsed time: 51 s.
 
 Done With Block Number:     2
 -----------------------------
 
 
 Valence Block Number:     3 of     3 || Pass     1 of     1
 -----------------------------------------------------------
 
 Using GPU support via OpenACC/OMP-Target implementation
 Using batch implementation, batch size:    20
 
 Started calculation of matrix elements with 84 transition(s) at 13:07:01.
 [ 13:07:01 |   0% ] transition  1 / 84.
 [ 13:07:02 |   2% ] transition  3 / 84, remaining: 13 s.
 [ 13:07:03 |  10% ] transition  9 / 84, remaining: 12 s.
 [ 13:07:04 |  19% ] transition 17 / 84, remaining: 10 s.
 [ 13:07:05 |  30% ] transition 26 / 84, remaining: 9 s.
 [ 13:07:07 |  39% ] transition 34 / 84, remaining: 8 s.
 [ 13:07:08 |  50% ] transition 43 / 84, remaining: 6 s.
 [ 13:07:09 |  60% ] transition 51 / 84, remaining: 5 s.
 [ 13:07:10 |  69% ] transition 59 / 84, remaining: 4 s.
 [ 13:07:12 |  80% ] transition 68 / 84, remaining: 3 s.
 [ 13:07:13 |  89% ] transition 76 / 84, remaining: 1 s.
 Finished calculation of matrix elements at 13:07:14.
 Elapsed time: 13 s.
 
 CHI summation GPU algorithm offloading all matrix elements to device.
 
 Started building polarizability matrix with 384 processor(s) at 13:07:15.
 [ 13:07:17 |   0% ] processor   1 / 384.
 [ 13:07:17 |   1% ] processor   3 / 384, remaining: 49 s.
 [ 13:07:22 |  10% ] processor  39 / 384, remaining: 43 s.
 [ 13:07:27 |  20% ] processor  77 / 384, remaining: 38 s.
 [ 13:07:31 |  30% ] processor 116 / 384, remaining: 33 s.
 [ 13:07:36 |  40% ] processor 154 / 384, remaining: 29 s.
 [ 13:07:41 |  50% ] processor 192 / 384, remaining: 24 s.
 [ 13:07:46 |  60% ] processor 231 / 384, remaining: 19 s.
 [ 13:07:51 |  70% ] processor 269 / 384, remaining: 15 s.
 [ 13:07:57 |  80% ] processor 308 / 384, remaining: 10 s.
 [ 13:08:02 |  90% ] processor 346 / 384, remaining: 5 s.
 [ 13:08:07 | 100% ] processor 384 / 384, remaining: 0 s.
 Finished building polarizability matrix at 13:08:07.
 Elapsed time: 52 s.
 
 Done With Block Number:     3
 -----------------------------
 
 q-pt      1: Head of Epsilon         =     1.416652250097134E+01    1.564413270735460E-17
 q-pt      1: Epsilon(2,2)            =     1.151002087971401E+01   -3.649278437613717E-18
 q-pt      1: Head of Epsilon Inverse =     7.899185187719440E-02   -1.002137583914119E-19
 q-pt      1: Epsilon Inverse(2,2)    =     9.665226569824223E-02    3.282252670610414E-20
 
 For q0 points, you should check the symmetry (eps(G,G') = eps*(-G,-G')) by
 using the eps0sym code. Wavefunction convergence, as well as a finite q-shift
 may cause this property of eps(G,G') to be broken.

 Writing dielectric matrix to file
 Ok


********************************************************************************
*                                                                              *
* Your calculation employed methods and algorithms published in peer-reviewed  *
* papers. Please, cite the following references to give proper credit to their *
*          authors and help support future development of BerkeleyGW.          *
*                                                                              *
********************************************************************************

 J. Deslippe, G. Samsonidze, D. A. Strubbe, M. Jain, M. L. Cohen, and S. G.
 Louie, BerkeleyGW: A Massively Parallel Computer Package for the Calculation
 of the Quasiparticle and Optical Properties of Materials and Nanostructures,
 Computer Physics Communications 183, 1269 (2012).

 M. S. Hybertsen and S. G. Louie, Electron Correlation in Semiconductors and
 Insulators: Band Gaps and Quasiparticle Energies, Phys. Rev. B 34, 5390
 (1986).

 M. D. Ben, C. Yang, Z. Li, F. H. da Jornada, S. G. Louie, and J. Deslippe,
 Accelerating Large-Scale Excited-State GW Calculations on Leadership HPC
 Systems, SC20: International Conference for High Performance Computing,
 Networking, Storage and Analysis (2020).

********************************************************************************

 
 Timing information
 
                                CPU time (s)          WALL time (s)       Number
 Routine                       min        max        min        max     of calls
 -------------------------  ---------  ---------  ---------  ---------  --------
 - Eps (I/O) Comm                0.06       1.92       0.06       1.92         1
 - Eps (I/O) IO                 57.54      59.38      57.54      59.38         1
 - I/O TOTAL                   115.23     115.38     115.23     115.38         8
 - INPUT                        39.22      39.24      39.22      39.24         1
 - INPUT_Q                      19.84      19.96      19.84      19.96         1
 - FULLBZ                        0.00       0.00       0.00       0.00         1
 - GVEC                          0.00       0.01       0.00       0.01         1
 - SUBGRP                        0.00       0.00       0.00       0.00         1
 - IRRBZ                         0.00       0.00       0.00       0.00         1
 - GENWF                         1.35       1.79       1.35       1.79       250
 - MTXEL                        36.84      39.13      36.83      39.12       250
 - RQSTAR                        0.00       0.00       0.00       0.00         3
 - GMAP                          0.01       0.01       0.01       0.01         3
 - EPSINV (TOTAL)               89.22      93.54      89.22      93.54         1
 - CHI SUM (COMM)               18.95      30.84      18.92      30.87      3456
 - CHI SUM (TOTAL)             152.23     158.90     152.23     158.91         3
 - GENWF (VAL)                   0.40       0.44       0.39       0.44       250
 - GENWF (COND)                  0.94       1.36       0.94       1.36         3
 - EPSINV (VCOUL)                0.00       4.32       0.00       4.32         1
 - JOB SETUP                     0.00       0.01       0.00       0.01         1
 - Q LOOP SETUP                  0.01       0.01       0.01       0.01         1
 - INIT CUTOFF                   0.00       0.00       0.00       0.00         1
 - INIT SCALAPACK                0.02       0.02       0.02       0.02         1
 - INIT ARRAYS                   3.24       3.99       3.25       3.99         4
 - MTXEL (DENOM)                 0.17       0.20       0.13       0.23    142500
 - MTXEL (FFT)                  36.84      39.13      36.83      39.12       250
 - GENWF (Ekin)                  0.01       0.01       0.00       0.01         6
 - GENWF (Sort)                  0.07       0.07       0.07       0.07         2
 - CHI SUM (zGEMM)              80.73      92.93      80.73      92.92      1152
 - CHI SUM (PREP)               30.44      32.75      30.42      32.76      1152
 - CHI SUM (ARRAY ALLOC)         2.44       3.17       2.44       3.23      1152
 - EPSINV (I/O)                 59.46      59.48      59.46      59.48         1
 - EPSINV (INVERT)              29.14      29.16      29.14      29.16         1
 - CHI SUM (BAR)                 0.50       5.77       0.50       5.77         3
 - FFT MLTPLY                    3.39       4.64       3.35       4.71    142500
 - TOTAL                       352.51     352.53     352.51     352.53         1
 
 Job Done
 

MPICH Slingshot Network Summary: 0 network timeouts

